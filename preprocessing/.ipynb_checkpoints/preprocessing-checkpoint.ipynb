{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'amsFeatures', u'label', u'label_scalar', u'ratemap']\n",
      "test_interval: 781.0\n"
     ]
    }
   ],
   "source": [
    "from nideep.datasets.balance_hdf5 import save_balanced_class_count_hdf5 \n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# train set:\n",
    "fpath_src = '/home/jiang/Documents/BA2016/data/split/twoears_data_train_000.h5'\n",
    "hdf5_file_name = fpath_src\n",
    "raw_data = h5py.File(hdf5_file_name, 'r')\n",
    "print raw_data.keys()\n",
    "# print raw_data['label_scalar'].shape\n",
    "# print raw_data['label'].shape\n",
    "# print raw_data['amsFeatures'].shape\n",
    "train_dataset_sz = raw_data['ratemap'].shape[0]\n",
    "train_batch_sz=128\n",
    "print 'test_interval:',round(train_dataset_sz/train_batch_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'amsFeatures', u'label', u'label_scalar', u'ratemap']\n",
      "36236\n"
     ]
    }
   ],
   "source": [
    "# test set:\n",
    "fpath_src = '/home/jiang/Documents/BA2016/data/split/twoears_data_test_001.h5'\n",
    "hdf5_file_name = fpath_src\n",
    "raw_data = h5py.File(hdf5_file_name, 'r')\n",
    "print raw_data.keys()\n",
    "# print raw_data['label_scalar'].shape\n",
    "# print raw_data['label'].shape\n",
    "# print raw_data['amsFeatures'].shape\n",
    "traw_data['ratemap'].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nideep.datasets.balance_hdf5 import save_balanced_class_count_hdf5 \n",
    "# train set:\n",
    "fpath_src = './data/data_train.h5'\n",
    "fpath_dst = './data/bal/data_train.h5' # parent directory must exist\n",
    "keys = ['feat1', 'feat2'] # make sure classnames are not included\n",
    "idxs = save_balanced_class_count_hdf5(fpath_src, keys, fpath_dst, key_label='label', other_clname='general')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nideep.iow.to_hdf5 import split_hdf5\n",
    "fpath_src = './data/data_train.h5'\n",
    "paths = split_hdf5(fpath_src, './data/split/')\n",
    "# ...create a txt file in which each line contains the absolute path of each new smaller HDF5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
