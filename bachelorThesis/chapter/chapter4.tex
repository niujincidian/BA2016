\chapter{Comparison of Optimization Algorithms}
%Discuss about different solver types(SGD,Adam,Adadelta,) related works
%"SGD"(Stochastic Gradient Descent)\\
%"AdaGrad"(Adaptive Gradient)\\
%"Adam"(Adaptive Moment Estimation)\\
%"AdaDelta"\\
%"NAG"(Nesterov’s accelerated gradient)\\
%"RMSprop"\\
\label{chap:solver}
In Caffe toolbox, optimization algorithms corresponds to the solver types. There are 6 types of solver implemented in Caffe:
\begin{itemize}
	\item Stochastic Gradient Descent (type: "SGD"),
\item	AdaDelta (type: "AdaDelta"),
\item	Adaptive Gradient (type: "AdaGrad"),
\item	Adam (type: "Adam"),
\item	Nesterov’s Accelerated Gradient (type: "Nesterov") and
\item	RMSprop (type: "RMSProp")
\end{itemize}
All the experiments carried out in previous sections used Stochastic Gradient Descent as the optimization algorithm.
 
\section{Implementation Details}

\section{Results and Discussion}
